# ðŸŒŠ CELEC Airflow Workflow Guide

Este documento explica cÃ³mo utilizar Apache Airflow para orquestar el pipeline completo de ML del proyecto CELEC sin afectar la funcionalidad existente.

## ðŸ“‹ QuÃ© es Airflow

Apache Airflow es una plataforma de orquestaciÃ³n que permite:
- **Programar** pipelines para ejecutarse automÃ¡ticamente
- **Monitorear** el estado de cada tarea
- **Gestionar dependencias** entre tareas
- **Reintentar** tareas fallidas automÃ¡ticamente
- **Paralelizar** tareas independientes

## ðŸŽ¯ Pipeline CELEC Implementado

El DAG `celec_flow_prediction_pipeline` orquesta las siguientes tareas:

```
1. check_environment       â†’ Verifica Python, MLflow, scikit-learn
2. validate_data           â†’ Valida datos de entrada
3. download_data           â†’ Ejecuta download_retrospective.py
4. run_basic_model         â†’ Ejecuta data_analysis.py (existente)
5. run_mlflow_model        â†’ Ejecuta run_with_mlflow.py (existente)
6. validate_results        â†’ Verifica que se generaron outputs
7. generate_report         â†’ Crea reporte del pipeline
8. notify_completion       â†’ Notifica finalizaciÃ³n
```

**âš ï¸ IMPORTANTE:** Los scripts existentes NO se modifican, solo se ejecutan vÃ­a Airflow.

## ðŸš€ Setup RÃ¡pido

### 1. Ejecutar ConfiguraciÃ³n AutomÃ¡tica
```bash
python airflow_setup.py
```

### 2. Iniciar Airflow

**Windows:**
```bash
start_airflow.bat
```

**Linux/Mac:**
```bash
./start_airflow.sh
```

### 3. Acceder a Dashboards

- **Airflow UI:** http://localhost:8080
  - Usuario: `admin`
  - ContraseÃ±a: `celec123`

- **MLflow UI:** http://localhost:5000

## ðŸ“Š Uso del Dashboard

### Activar el Pipeline
1. Ir a http://localhost:8080
2. Buscar DAG `celec_flow_prediction_pipeline`
3. Activar el toggle (OFF â†’ ON)
4. El pipeline se ejecutarÃ¡ diariamente a las 6 AM

### Ejecutar Manualmente
1. Click en el DAG name
2. Click "Trigger DAG" (botÃ³n â–¶ï¸)
3. Confirmar ejecuciÃ³n

### Monitorear Progreso
- **Graph View:** Ver dependencias entre tareas
- **Tree View:** Ver ejecuciones histÃ³ricas
- **Logs:** Click en cualquier tarea para ver logs detallados

## âš™ï¸ ConfiguraciÃ³n del Pipeline

### ProgramaciÃ³n
El pipeline se ejecuta:
- **Frecuencia:** Diaria a las 6:00 AM
- **Zona horaria:** UTC
- **Reintentos:** 2 intentos por tarea fallida
- **Delay entre reintentos:** 5 minutos

### Modificar ProgramaciÃ³n
En `dags/celec_ml_pipeline.py`, lÃ­nea 24:
```python
schedule_interval='0 6 * * *',  # Cambiar formato cron
```

Ejemplos:
- `'0 */6 * * *'` - Cada 6 horas
- `'0 0 * * 1'` - Cada lunes
- `None` - Solo ejecuciÃ³n manual

## ðŸ”§ PersonalizaciÃ³n

### AÃ±adir Tareas
```python
new_task = BashOperator(
    task_id='nueva_tarea',
    bash_command='python mi_script.py',
    dag=dag,
)

# Definir dependencias
existing_task >> new_task >> next_task
```

### Configurar Notificaciones Email
En el DAG, actualizar:
```python
email_on_failure = EmailOperator(
    to=['tu-email@ejemplo.com'],  # Cambiar email
    # ... resto de configuraciÃ³n
)
```

### Variables de Ambiente
AÃ±adir en `.env.airflow`:
```bash
MI_VARIABLE=mi_valor
```

Usar en DAG:
```python
import os
mi_valor = os.getenv('MI_VARIABLE')
```

## ðŸ” Troubleshooting

### Pipeline Falla
1. **Ver logs:** Click en tarea fallida â†’ "Log"
2. **Verificar datos:** Â¿Existe `data/raw/620883808_retrospective_data.csv`?
3. **Verificar ambiente:** Â¿MLflow instalado correctamente?
4. **Reintentar:** Click "Clear" en tarea fallida

### Docker Issues
```bash
# Ver contenedores corriendo
docker ps

# Ver logs de servicio especÃ­fico
docker-compose -f docker-compose.airflow.yml logs airflow-webserver

# Reiniciar servicios
stop_airflow.bat  # o ./stop_airflow.sh
start_airflow.bat # o ./start_airflow.sh
```

### Permisos (Linux/Mac)
```bash
# Si hay problemas de permisos
sudo chown -R $USER:$USER logs/
chmod 755 start_airflow.sh stop_airflow.sh
```

## ðŸ“ˆ Monitoreo y MÃ©tricas

### Dashboard de Estado
- **Success Rate:** Porcentaje de ejecuciones exitosas
- **Duration:** Tiempo promedio de ejecuciÃ³n
- **Last Run:** Estado de la Ãºltima ejecuciÃ³n

### IntegraciÃ³n con MLflow
- Todos los experimentos se registran automÃ¡ticamente
- MÃ©tricas disponibles en http://localhost:5000
- Modelos versionados en el registry

### Logs Centralizados
- Todos los logs en `logs/` directory
- Organizados por DAG, tarea y fecha
- BÃºsqueda integrada en Airflow UI

## ðŸ”„ Workflows Avanzados

### Pipeline de Re-entrenamiento
```python
# Trigger cuando hay nuevos datos
file_sensor = FileSensor(
    task_id='wait_for_new_data',
    filepath='data/raw/new_data_flag.txt',
    dag=dag,
)

file_sensor >> run_mlflow_model_task
```

### ValidaciÃ³n de Modelo
```python
# Comparar con modelo anterior
model_validation_task = PythonOperator(
    task_id='validate_model_performance',
    python_callable=compare_model_metrics,
    dag=dag,
)
```

### Deployment Condicional
```python
# Deploy solo si R2 > 0.95
deploy_task = BashOperator(
    task_id='deploy_model',
    bash_command='python deploy_model.py',
    dag=dag,
    trigger_rule='all_success',  # Solo si todas las tareas previas pasaron
)
```

## ðŸ›‘ Detener Servicios

**Windows:**
```bash
stop_airflow.bat
```

**Linux/Mac:**
```bash
./stop_airflow.sh
```

## ðŸ“š Recursos Adicionales

- [DocumentaciÃ³n Oficial de Airflow](https://airflow.apache.org/docs/)
- [Tutorial de DAGs](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- [Operators Reference](https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref/index.html)

## âœ… VerificaciÃ³n de Funcionalidad

Para confirmar que todo funciona correctamente:

1. **Scripts Existentes:** 
   ```bash
   # Estos DEBEN seguir funcionando independientemente
   python src/models/data_analysis.py
   python run_with_mlflow.py
   python start_mlflow_ui.py
   ```

2. **Pipeline de Airflow:**
   ```bash
   # Abrir http://localhost:8080
   # Ejecutar DAG manualmente
   # Verificar que todas las tareas pasen
   ```

3. **Outputs Generados:**
   ```bash
   # Verificar que se crean estos archivos
   ls data/processed/model_predictions.csv
   ls models/trained_model.pkl
   ls reports/figures/*.png
   ```

---

**ðŸ”‘ Punto Clave:** Airflow es una capa de orquestaciÃ³n que NO modifica el cÃ³digo existente. Todos los scripts originales siguen funcionando exactamente igual, pero ahora tambiÃ©n pueden ejecutarse de forma programada y monitoreada travÃ©s de Airflow.